# Paper List
Paper reading list in LLMs: 
- [Efficient Large Language Model](#Efficient-Large-Language-Model)
- [Understand LLMs](#Understand-LLMs)
  - [Model Structure](#model-structure)
  - [Gradient Approximate](#gradient-approximate)
  - [LLM as latent variable Model](#LLM-as-latent-variable-Model)
  - [Training Data](#training-data)
  - [Multiple skills](#multiple-skills)
- [Reasoning in LLMs](#Reasoning-in-LLMs)
- [Causal Inference](#Causal-Inference)
- [Cognitive LLMs](#cognitive-llm)

This repository will keep updating ... ðŸ¤—
***


## Efficient Large Language Model
* Fast inference from transformers via speculative decoding. ICLR23 oral. [paper](https://arxiv.org/abs/2211.17192)
* SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification. [paper](https://arxiv.org/pdf/2305.09781.pdf)

ðŸ‘† [Back to Top](#paper-list)

## Understand LLMs
**Tracr**: TRAnsformer Compiler for RASP

### Model Structure
* In-context Learning and Induction Heads (Anthropic AI) [paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
* Transformers as Algorithms: Generalization and Stability in In-context Learning. ICML23. [paper](https://arxiv.org/pdf/2301.07067.pdf)
  - Prerequisite reading
    - A Mathematical Framework for Transformer Circuits. [paper](https://transformer-circuits.pub/2021/framework/index.html#three-kinds-of-composition)
### Gradient Approximate
* Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers2022. [paper](https://arxiv.org/abs/2212.10559)
* What LEARNING ALGORITHM IS IN-CONTEXT LEARNING? INVESTIGATIONS WITH LINEAR MODELS. ICLR23. [paper](https://arxiv.org/abs/2211.15661)
* Transformers Learn In-Context by Gradient Descent. ICML23. [paper](https://arxiv.org/abs/2212.0767)
### LLM as latent variable Model
* An Explanation of In-context Learning as Implicit Bayesian Inference. [paper](https://arxiv.org/abs/2111.02080)
* Schema-learning and rebinding as mechanisms of in-context learning and emergence. DeepMind, June23. [paper](https://arxiv.org/pdf/2307.01201.pdf)
  - Propose a sequence learning model based on action->latent variable->observed variable Generation process.
### Training Data
* Data Distributional Properties Drive Emergent In-Context Learning in Transformers. [paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf)
* Pretraining task diversity and the emergence of non-Bayesian in-context learning for regressionâ€‹. [paper](proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf)
### Multiple skills
* A Theory for Emergence of Complex Skills in Language Models. [paper](â€‹https://arxiv.org/pdf/2307.15936.pdf)

ðŸ‘† [Back to Top](#paper-list)
## Causal Inference
* Causal interventions expose implicit situation models for commonsense language understanding. ACL-findings23. [paper](https://arxiv.org/pdf/2306.03882.pdf)

ðŸ‘† [Back to Top](#paper-list)
## Reasoning in LLMs
* Make a Choice! Knowledge Base Question Answering with In-Context Learning
* STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning.  Neurips22, cite110+. [paper](https://â€‹openreview.net/forum?id=_3ELRdg2sgI)
* ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models.[paper](https://arxiv.org/pdf/2303.16421.pdf)

ðŸ‘† [Back to Top](#paper-list)
## Cognitive LLMs
* Dissociating Language and thought in large Language Model: a cognitive perspective. [paper](https://arxiv.org/pdf/2301.06627.pdf)
  
ðŸ‘† [Back to Top](#paper-list)
