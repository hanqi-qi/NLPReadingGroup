# Paper List
Paper reading list in ğŸ’¬ **Efficient Large Language Model** and ğŸ“ **Understand LLMs**. This repository will keep updating ... ğŸ¤—

- [Efficient Large Language Model](#Efficient-Large-Language-Model)
- [Understand LLMs](#Understand-LLMs)
  - [Model Structure](#model-structure)
  - [Gradient Approximate](#gradient-approximate)
  - [LLM as latent variable Model](#LLM-as-latent-variable-Model)
- [Reasoning in LLMs](#Reasoning-in-LLMs)
- [Causal Inference in LLMs](#Causal-Inference)
***


## Efficient Large Language Model
* Fast inference from transformers via speculative decoding.,ICLR23 oral. [Paper](https://arxiv.org/abs/2211.17192)
* SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification. [Paper](https://arxiv.org/pdf/2305.09781.pdf)

ğŸ‘† [Back to Top](#paper-list)

## Understand LLMs
**Tracr**: TRAnsformer Compiler for RASP

### Model Structure
* Refer to my another blog "æ³¨æ„åŠ›æœºåˆ¶ï¼ˆinduction head) contribute to In-context Learning"
hanqi-qiï¼šæ³¨æ„åŠ›æœºåˆ¶ï¼ˆinduction head) contribute to In-context Learning

[1] In-context Learning and Induction Heads (Anthropic AI)
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
â€‹transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html

Other researches about Attention in Transformersï¼š

[2]Transformers as Algorithms: Generalization and Stability in In-context Learning. ICML23
https://arxiv.org/pdf/2301.07067.pdf
â€‹arxiv.org/pdf/2301.07067.pdf
contributions:
ï¼ˆaï¼‰ICL å¯ä»¥è¢«é‡æ–°frame æˆmultiple task ï¼ˆMTLï¼‰çš„ä»»åŠ¡ï¼Œå¹¶ä¸”ä»sequence dataä¸­å­¦åˆ°full tasksçš„çŸ¥è¯†
ï¼ˆbï¼‰attentionæœºåˆ¶ä¿è¯äº†åœ¨MTLæ¡†æ¶ä¸‹çš„generazability
  (c)   MTLçš„generalizability bound å¯ä»¥æ¨å¹¿åˆ°meta-learning/transfer learningçš„æ¡†æ¶ä¸‹

Prerequisite reading:
A Mathematical Framework for Transformer Circuits
https://transformer-circuits.pub/2021/framework/index.html#three-kinds-of-composition
â€‹transformer-circuits.pub/2021/framework/index.html#three-kinds-of-composition
Key concepts:
ã€1ã€‘Privileged basis:
Residual stream to have â€œno privileged basis. By this we mean that there is no reason to expect the individual coordinates in the stream to have any particular meaning or significant property at all. This belief arises from the observation that every operation that reads from or writes to the residual stream does so via an arbitrary full-rank linear transformation. That in turn implies that we could transform the residual stream by an arbitrary full-rank linear transformation, and then also multiply the same transformation into every other matrix in the Transformer in the appropriate way, and arrive at an identical function with completely different coordinates.

### Gradient Descent

Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers2022

WHAT LEARNING ALGORITHM IS IN-CONTEXT LEARNING? INVESTIGATIONS WITH LINEAR MODELS. ICLR23
è¦æ³¨æ„çš„æ˜¯ä¸¤ç§ä¸åŒçš„è®¾å®šï¼šä¸€ç§æ˜¯single self-attention layeråœ¨ICLsamplesä¸Šçš„trained ä¹‹åï¼Œå®ƒçš„outputç­‰åŒäºç­‰åŒäºlinear modelè¿›è¡Œæ¢¯åº¦ä¼˜åŒ–çš„ç»“æœã€‚ å¦å¤–çš„æ˜¯ä¸€ç§æ˜¯ï¼šTransformer(TF) train on ICL objective but linear regression dataï¼ˆä¸åŒçš„åˆ†å¸ƒè§ä»¥ä¸‹4ç§ï¼‰ï¼Œå¯¹ä¸åŒlinear modelçš„æ‹Ÿåˆç¨‹åº¦(å…¶ä¸­ è®¾è®¡äº†SFD, ILWDä¸¤ç§è®¡ç®—output å’Œweight åå·®çš„æŒ‡æ ‡æ¥è¡¡é‡TFå¯¹è¿™äº›linear predictorçš„æ‹Ÿåˆç¨‹åº¦)ï¼šåŒ…æ‹¬ï¼š
 k-neareset 
one-step stochastic optimization 
ridge-regression 
one-batch gradient descent. 
è¿™äº›å®éªŒéƒ½æ˜¯åœ¨ä»¿çœŸæ•°æ®ä¸Šè¿›è¡Œçš„ï¼Œx, w å‡é‡‡æ ·äºN(0,I), yæ˜¯é€šè¿‡ä»¥ä¸Šä¸åŒæ–¹æ³•è®¡ç®—å¾—åˆ°çš„ã€‚
Transformers Learn In-Context by Gradient Descent. ICML23

ç›¸æ¯”äºå‰ä¸¤ç¯‡æ–‡ç« ï¼Œè¿™ç¯‡æ–‡ç« distinguishedçš„è´¡çŒ®ç‚¹ä¹Ÿä¸å°‘ï¼Œæ€ªä¸å¾—å¼•ç”¨è¿™ä¹ˆå¤šã€‚ä¹ä¸€çœ‹ï¼Œéƒ½æ˜¯è¯´LSAï¼ˆlinear self-attentionï¼‰(linear attention layer åœ¨in-context dataä¸Šè®­ç»ƒçš„ç»“æœçš„ç»“æœå¯ä»¥è¿‘ä¼¼äº linear layeråšä¸€æ¬¡GDçš„ç»“æœï¼ˆç†è®ºè¯´æ˜ï¼‰ï¼Œä¸”åœ¨linear regressionçš„taskä¸Šå¯ä»¥å¾—åˆ°è¿‘ä¼¼ç»“æœï¼ˆå®éªŒè¯´æ˜ï¼‰ã€‚ç»“æœå½“ç„¶å•¦ï¼ŒICLR23è¿™ç¯‡æ–‡ç« è¯´çš„æ˜¯æœ‰MLPçš„self-attentionï¼‰,ä½†æ–°çš„åœ°æ–¹åœ¨äºï¼š
ï¼ˆ1ï¼‰ æå‡ºè¦åˆ¤æ–­ä¸¤ä¸ªmodel algorithm æ˜¯ä¸æ˜¯equivalent, ä¸èƒ½åªçœ‹ä»–ä»¬çš„predictionæ˜¯å¦ä¸€è‡´ï¼Œè¿˜è¦çœ‹(a)interpolation (b) oov result (c) repeat GDå¤šæ¬¡æ˜¯å¦èƒ½å¾—åˆ°ç±»ä¼¼ç»“æœã€‚

ï¼ˆ2ï¼‰åœ¨LSAä¹‹å‰å¼•å…¥MLPï¼Œ å¯ä»¥å»åšéçº¿æ€§çš„ä»»åŠ¡çš„æ‹Ÿåˆï¼Œæ¯”å¦‚sinWave.

(3) ç»“åˆinduction-headä¸­çš„å‘ç°ï¼Œå†ä¸€æ¬¡éªŒè¯äº†one-layer attention ï¼ˆcopy) å’Œtwo-layer attention(induction head) åˆ†åˆ«çš„ä½œç”¨ ã€‚è¿™æ˜¯æˆ‘ä¸ªäººæ¯”è¾ƒå…³æ³¨çš„é‡ç‚¹ã€‚å…·ä½“çº¿ç´¢å¦‚ä¸‹ï¼š

åœ¨æ¥è¿‘GDæ•ˆæœä¹‹å‰ï¼Œå‚ç…§å·¦å›¾ åœ¨training step2000-3000ä¹‹é—´ï¼Œç¬¬ä¸€å±‚çš„è¾“å‡ºï¼ˆå‚ç…§å³å›¾ï¼‰ï¼Œä¹Ÿå°±æ˜¯å½“å‰tokençš„representation e_j ä¸ä¸‹ä¸€ä¸ªtokençš„e_{j+1}å¯†åˆ‡ç›¸å…³ã€‚é‚£ä¹ˆç¬¬äºŒå±‚åˆ™æ˜¯ä¸GDç›¸å…³ã€‚
é‚£ä¹ˆcopyæ˜¯è¿›è¡Œgradient descent çš„å‰ææ¡ä»¶ï¼Œå¹¶ä¸”è¿™ä¸ªcopyæ˜¯é€šè¿‡softmaxçš„å®ç°ï¼šè®¡ç®—å½“å‰token ä¸å…¶å®ƒtokençš„correlationçš„ç›¸å…³ï¼Œå…¶å®å°±æ˜¯è®°å½•å½“å‰token attentionå€¼çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªåœ¨æˆ‘ä¹‹å‰è¯¦è§£induction head circuitçš„blogä¹Ÿæœ‰æåˆ°ã€‚

### LLM as latent variable Model

AN EXPLANATION OF IN-CONTEXT LEARNING AS IMPLICIT BAYESIAN INFERENCE. cite160+ 2021
è¿™æ˜¯ç¬¬ä¸€ç¯‡å¹¿ä¸ºæµä¼ çš„ç”¨latent variable modelè§£é‡Šå¤§æ¨¡å‹in-context learningçš„æ–‡ç« 
An Explanation of In-context Learning as Implicit Bayesian Inference
â€‹arxiv.org/abs/2111.02080

Schema-learning and rebinding as mechanisms of in-context learning and emergence. DeepMind, June23
TL;DR: Propose a sequence learning model based on action->latent variable->observed variable Generation process.
https://arxiv.org/pdf/2307.01201.pdf
â€‹arxiv.org/pdf/2307.01201.pdf

### Training Data
Data Distributional Properties Drive Emergent In-Context Learning in Transformers
https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf

Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression
â€‹proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf


**(E) multiple skills**
A Theory for Emergence of Complex Skills in Language Models
â€‹arxiv.org/pdf/2307.15936.pdf

## Causal Inference
å…¶å®å¾ˆå¤šæ–‡ç« è·Ÿprinciple çš„causal inferenceå…³ç³»æ²¡é‚£ä¹ˆç´§å¯†ï¼Œå°±æ˜¯ç”¨äº†ä¸€ä¸‹causal çš„å‡ ä¸ªé‡è¦æ¦‚å¿µã€‚ä½†ä¹Ÿç®—æ˜¯ä¸€ä¸ªæµæ´¾å§ã€‚
Causal interventions expose implicit situation models for commonsense language understanding. ACL-findings 23
https://arxiv.org/pdf/2306.03882.pdf

â€‹arxiv.org/pdf/2306.03882.pdf

## Reasoning-in-LLMs
Make a Choice! Knowledge Base Question Answering with In-Context Learning

STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning.  Neurips22, cite110+

STaR: Bootstrapping Reasoning With Reasoning
â€‹openreview.net/forum?id=_3ELRdg2sgI

ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models.
https://arxiv.org/pdf/2303.16421.pdf

â€‹arxiv.org/pdf/2303.16421.pdf

**Cognitive Perspective**
https://arxiv.org/pdf/2301.06627.pdf

â€‹arxiv.org/pdf/2301.06627.pdf
